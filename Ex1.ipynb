{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66b239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes --prefix {sys.prefix} numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e78321ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class softmax_layer:\n",
    "    def dim_in(self):\n",
    "        return 0\n",
    "    \n",
    "    def dim_out(self):\n",
    "        return 0\n",
    "    \n",
    "    def update_weigths(self, gradient, learning_rate):\n",
    "        return gradient\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        exp = np.exp(x.T - np.max(x, axis=1))\n",
    "        return (exp / np.sum(exp, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3ccda364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self._A = np.random.rand(dim_in,dim_out)\n",
    "        self._B = np.random.rand(dim_out)\n",
    "        self._dim_in = dim_in\n",
    "        self._dim_out = dim_out\n",
    "        \n",
    "    def update_weigths(self, gradient, learning_rate):\n",
    "        self._A -= learning_rate * gradient\n",
    "        return gradient\n",
    "    \n",
    "    def dim_in(self):\n",
    "        return self._dim_in\n",
    "    \n",
    "    def dim_out(self):\n",
    "        return self._dim_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x.T.dot(self._A) + self._B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a3eeaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_predicted, epsilon=1e-10):\n",
    "    predictions = np.clip(y_predicted, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    return -np.sum(y_true * np.log(predictions)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "29738625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_model:\n",
    "    def __init__(self, *layers, learning_rate=0.01):\n",
    "        self._learning_rate = learning_rate\n",
    "        self._layers = []\n",
    "        last_dim_out = 0\n",
    "        for layer in layers:\n",
    "            if last_dim_out != 0 and layer.dim_in() != 0 and last_dim_out != layer.dim_in():\n",
    "                print('dimension dont match layer out dim {} , next layer dim in {}'.format(last_dim_out, layer.dim_in()))\n",
    "                raise \n",
    "            self._layers.append(layer)\n",
    "            if layer.dim_out() != 0:\n",
    "                last_dim_out = layer.dim_out()\n",
    "                \n",
    "    def update_weigths(self, gradient):\n",
    "        for layer in reversed(self._layers):\n",
    "            gradient = layer.update_weigths(gradient, self._learning_rate)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9bde65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential_model(\n",
    "        linear_layer(2, 2),\n",
    "        softmax_layer()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "94d459a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.1\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "beb0c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('SwissRollData.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5726d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20000)\n"
     ]
    }
   ],
   "source": [
    "X = mat['Yt']\n",
    "Y = mat['Ct']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a11ce3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20000)\n",
      "(2, 20000)\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model(X)\n",
    "print(Y.shape)\n",
    "print(Y_predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "26cbb0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7650.525471345105\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy_loss(Y, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c7ccec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_gradient(X, Y_true, Y_predicted):\n",
    "    M = X.shape[0]\n",
    "    return 1/M * X.dot((Y_true - Y_predicted).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e93d041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 116.2355859  -116.2355859 ]\n",
      " [  74.67569786  -74.67569786]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_gradient(X, Y, Y_predicted))\n",
    "\n",
    "def SGD(model, X, Y):\n",
    "    Y_predicted = model(X)\n",
    "    gradient = softmax_gradient(X, Y, Y_predicted)\n",
    "    model.update_weigths(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "217dc1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bla\n",
      "bla\n",
      "[[0.29429512 0.51109954]\n",
      " [0.67999543 0.54747531]]\n",
      "[[-0.86806074  1.6734554 ]\n",
      " [-0.06676155  1.29423229]]\n"
     ]
    }
   ],
   "source": [
    "SGD(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4bc6cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69644951 0.30355049]\n",
      " [0.68110041 0.31889959]\n",
      " [0.69554053 0.30445947]\n",
      " ...\n",
      " [0.09410854 0.90589146]\n",
      " [0.09259983 0.90740017]\n",
      " [0.1115786  0.8884214 ]]\n",
      "9503.22830217776\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model(X)\n",
    "print(Y_predicted.T)\n",
    "print(cross_entropy_loss(Y, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ba5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
