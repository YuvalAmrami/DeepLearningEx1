{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3dfa19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes --prefix {sys.prefix} numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e712dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class softmax_layer:\n",
    "    def dim_in(self):\n",
    "        return 0\n",
    "    \n",
    "    def dim_out(self):\n",
    "        return 0\n",
    "    \n",
    "    def update_weigths(self, gradient, learning_rate):\n",
    "        return gradient\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        #exp = np.exp(x.T - np.max(x, axis=1))\n",
    "        exp = np.exp(x)\n",
    "        return (exp.T / np.sum(exp, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624e5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_layer:\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        self._A = np.random.rand(dim_in,dim_out)\n",
    "        self._B = np.random.rand(dim_out)\n",
    "        self._dim_in = dim_in\n",
    "        self._dim_out = dim_out\n",
    "        \n",
    "    def update_weigths(self, gradient, learning_rate):\n",
    "        self._A = self._A - learning_rate * gradient\n",
    "        return gradient\n",
    "    \n",
    "    def dim_in(self):\n",
    "        return self._dim_in\n",
    "    \n",
    "    def dim_out(self):\n",
    "        return self._dim_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x.T.dot(self._A) + self._B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9406352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_predicted, epsilon=1e-10):\n",
    "    predictions = np.clip(y_predicted, epsilon, 1. - epsilon)\n",
    "    M = predictions.shape[1]\n",
    "    return -np.sum(y_true * np.log(predictions)) / M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab82946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequential_model:\n",
    "    def __init__(self, *layers, learning_rate=0.1):\n",
    "        self._learning_rate = learning_rate\n",
    "        self._layers = []\n",
    "        last_dim_out = 0\n",
    "        for layer in layers:\n",
    "            if last_dim_out != 0 and layer.dim_in() != 0 and last_dim_out != layer.dim_in():\n",
    "                print('dimension dont match layer out dim {} , next layer dim in {}'.format(last_dim_out, layer.dim_in()))\n",
    "                raise \n",
    "            self._layers.append(layer)\n",
    "            if layer.dim_out() != 0:\n",
    "                last_dim_out = layer.dim_out()\n",
    "                \n",
    "    def update_weigths(self, gradient):\n",
    "        for layer in reversed(self._layers):\n",
    "            gradient = layer.update_weigths(gradient, self._learning_rate)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806278da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential_model(\n",
    "        linear_layer(2, 2),\n",
    "        softmax_layer()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9ee1eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.1\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --prefix {sys.prefix} scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5fff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('SwissRollData.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344319e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20000)\n"
     ]
    }
   ],
   "source": [
    "X = mat['Yt']\n",
    "Y = mat['Ct']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afc4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20000)\n",
      "(2, 20000)\n",
      "[[0.4050652  0.5949348 ]\n",
      " [0.40266708 0.59733292]\n",
      " [0.40659561 0.59340439]\n",
      " ...\n",
      " [0.43022843 0.56977157]\n",
      " [0.42512818 0.57487182]\n",
      " [0.4395     0.5605    ]]\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model(X)\n",
    "print(Y.shape)\n",
    "print(Y_predicted.shape)\n",
    "print(Y_predicted.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea69d436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7123076369326017\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy_loss(Y, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4533a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_gradient(X, Y_true, Y_predicted):\n",
    "    M = X.shape[1]\n",
    "    grad = Y_predicted - Y_true\n",
    "    return 1/M * X.dot((Y_predicted - Y_true).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36328d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_true, Y_predicted):\n",
    "    return np.sum(np.argmax(Y_true, axis=0) == np.argmax(Y_predicted, axis=0)) / Y_true.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd42a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00666356 -0.00666356]\n",
      " [-0.01158237  0.01158237]]\n",
      "0.49465\n"
     ]
    }
   ],
   "source": [
    "print(softmax_gradient(X, Y, Y_predicted))\n",
    "print(accuracy(Y, Y_predicted))\n",
    "\n",
    "def SGD(model, X, Y, epoch):\n",
    "    loss = []\n",
    "    accuracy_sgd = []\n",
    "    for i in range(epoch):\n",
    "        Y_predicted = model(X)\n",
    "        gradient = softmax_gradient(X, Y, Y_predicted)\n",
    "        model.update_weigths(gradient)\n",
    "        accuracy_sgd.append(accuracy(Y, Y_predicted))\n",
    "        loss.append(cross_entropy_loss(Y, Y_predicted))\n",
    "    return loss, accuracy_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c24d4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7123076369326017,\n",
       "  0.7122721407473724,\n",
       "  0.712237492960379,\n",
       "  0.712203673312955,\n",
       "  0.7121706620270722,\n",
       "  0.7121384397941416,\n",
       "  0.7121069877640637,\n",
       "  0.7120762875345245,\n",
       "  0.7120463211405312,\n",
       "  0.7120170710441819,\n",
       "  0.7119885201246648,\n",
       "  0.7119606516684864,\n",
       "  0.7119334493599155,\n",
       "  0.7119068972716474,\n",
       "  0.711880979855678,\n",
       "  0.7118556819343861,\n",
       "  0.7118309886918178,\n",
       "  0.7118068856651703,\n",
       "  0.711783358736471,\n",
       "  0.7117603941244454,\n",
       "  0.7117379783765726,\n",
       "  0.7117160983613225,\n",
       "  0.7116947412605726,\n",
       "  0.7116738945621982,\n",
       "  0.7116535460528337,\n",
       "  0.7116336838108019,\n",
       "  0.7116142961992061,\n",
       "  0.7115953718591822,\n",
       "  0.7115768997033067,\n",
       "  0.7115588689091603,\n",
       "  0.7115412689130374,\n",
       "  0.711524089403804,\n",
       "  0.7115073203168991,\n",
       "  0.7114909518284754,\n",
       "  0.7114749743496753,\n",
       "  0.7114593785210442,\n",
       "  0.7114441552070708,\n",
       "  0.7114292954908569,\n",
       "  0.711414790668913,\n",
       "  0.711400632246074,\n",
       "  0.711386811930536,\n",
       "  0.7113733216290086,\n",
       "  0.7113601534419814,\n",
       "  0.711347299659103,\n",
       "  0.7113347527546674,\n",
       "  0.711322505383207,\n",
       "  0.7113105503751906,\n",
       "  0.7112988807328224,\n",
       "  0.7112874896259394,\n",
       "  0.7112763703880068,\n",
       "  0.7112655165122089,\n",
       "  0.7112549216476293,\n",
       "  0.711244579595526,\n",
       "  0.7112344843056907,\n",
       "  0.7112246298728975,\n",
       "  0.7112150105334338,\n",
       "  0.7112056206617146,\n",
       "  0.7111964547669765,\n",
       "  0.7111875074900518,\n",
       "  0.7111787736002161,\n",
       "  0.7111702479921153,\n",
       "  0.7111619256827607,\n",
       "  0.7111538018086014,\n",
       "  0.7111458716226596,\n",
       "  0.711138130491741,\n",
       "  0.7111305738937066,\n",
       "  0.7111231974148116,\n",
       "  0.7111159967471085,\n",
       "  0.7111089676859101,\n",
       "  0.711102106127315,\n",
       "  0.7110954080657907,\n",
       "  0.7110888695918155,\n",
       "  0.7110824868895761,\n",
       "  0.7110762562347208,\n",
       "  0.7110701739921649,\n",
       "  0.7110642366139514,\n",
       "  0.7110584406371592,\n",
       "  0.7110527826818649,\n",
       "  0.711047259449151,\n",
       "  0.7110418677191624,\n",
       "  0.7110366043492099,\n",
       "  0.7110314662719183,\n",
       "  0.7110264504934201,\n",
       "  0.7110215540915902,\n",
       "  0.7110167742143263,\n",
       "  0.711012108077867,\n",
       "  0.711007552965152,\n",
       "  0.7110031062242218,\n",
       "  0.7109987652666556,\n",
       "  0.7109945275660456,\n",
       "  0.7109903906565089,\n",
       "  0.7109863521312351,\n",
       "  0.7109824096410683,\n",
       "  0.7109785608931235,\n",
       "  0.710974803649436,\n",
       "  0.7109711357256429,\n",
       "  0.7109675549896971,\n",
       "  0.7109640593606117,\n",
       "  0.7109606468072333,\n",
       "  0.7109573153470471],\n",
       " [0.49465,\n",
       "  0.4961,\n",
       "  0.49735,\n",
       "  0.49805,\n",
       "  0.4986,\n",
       "  0.499,\n",
       "  0.4993,\n",
       "  0.49935,\n",
       "  0.4994,\n",
       "  0.4994,\n",
       "  0.4994,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945,\n",
       "  0.49945])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD(model, X, Y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0bfe704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4057926  0.5942074 ]\n",
      " [0.40430412 0.59569588]\n",
      " [0.40658706 0.59341294]\n",
      " ...\n",
      " [0.4093695  0.5906305 ]\n",
      " [0.40665841 0.59334159]\n",
      " [0.41478353 0.58521647]]\n",
      "0.7109540630450099\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model(X)\n",
    "print(Y_predicted.T)\n",
    "print(cross_entropy_loss(Y, Y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1eefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
